{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jagainu/SystemML_Usage/blob/master/lecture2_exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dzNnr2-4yS8"
      },
      "source": [
        "# 2023年 大規模言語モデル サマースクール 第2回演習\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5jvA8po4yTB"
      },
      "source": [
        "## 演習の目的\n",
        "「学習済みのLLM を（追加学習せずに）活用する技術について学ぶ」という目的のもと以下の目標を設定して講義を行いました。\n",
        "- Prompting やIn-Context Learning とはなにか説明できるようになる。\n",
        "- Augmented Language Model とはなにか説明できるようになる。\n",
        "\n",
        "本演習では、Hugging Face を使った公開モデルを使った基本的なPromptingの実装、Augmented Language Models の1 種であるRetrieval Augmented Generation の実装を行います。\n",
        "\n",
        "## 目次\n",
        "- Prompting\n",
        "    - 公開モデルをHuggingFace経由で使用\n",
        "    - （補足) API経由でのみ利用可能なモデルを使用\n",
        "    - Zero-shot / few-shot / CoT promptingでpromptingの効果を実感\n",
        "    - Prompt engineering guide\n",
        "- Retrieval Augmented Generation\n",
        "    - Generation without Retrieval\n",
        "    - Generation with gold passage\n",
        "    - Retrieval の実装\n",
        "    - Retrieval Augmented Generation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yhu1xcGfv05Q"
      },
      "source": [
        "## Prompting\n",
        "タスクに合わせてpromptを設計することで、追加の学習コストなしで大規模モデルを活用してタスクを解かせることができます。\n",
        "重みにアクセスできないがAPI経由で使用できるClosed Model、重みにアクセスできるOpen Modelを用いてzero-shot / few-shot / CoT Promptingを試してみましょう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RYW_Rcyv05Q"
      },
      "source": [
        "### 公開モデルをHuggingFace経由で使用\n",
        "API経由で使用できる大規模言語モデルは通常モデルの重みにはアクセスできず、自前データでの学習やモデルの分析などには限界があります。\n",
        "研究機関や企業がモデルの重みを公開することもあり、1個人であっても大規模に訓練されたモデルの重みを活用して自由に使用することができます。(商用利用など特定の用途はライセンスによって制限される場合があります。)\n",
        "\n",
        "先日Meta社から公開されたLlama2モデルは以下のリンクからリクエストを送信することで重みのダウンロードリンクを得ることができ、指定のライセンスのもと使用することができます。  \n",
        "https://ai.meta.com/resources/models-and-libraries/llama-downloads/  \n",
        "https://github.com/facebookresearch/llama  \n",
        "\n",
        "\n",
        "HuggingFaceというプラットフォーム上で公開されることも多くあり、演習ではtransformersというライブラリを使用してHuggingFace上に公開されているモデルを使用してみます。  \n",
        "https://huggingface.co/  \n",
        "https://huggingface.co/docs/transformers/index  \n",
        "HuggingFace上に公開されているモデルをtransformersライブラリを使用しロードし、ロードしたモデルにpromptを入れて出力を確認しましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1cCiORbv05R"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "# HuggingFaceにアップロードされたモデルやトークナイザーを使うためのライブラリ\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# https://huggingface.co/rinna/bilingual-gpt-neox-4b\n",
        "# https://huggingface.co/rinna/bilingual-gpt-neox-4b/tree/main\n",
        "# from_pretrainedの引数にモデル名を指定すると、モデルをダウンロードしてきてくれます。ダウンロードには3分ほどかかります。\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"rinna/bilingual-gpt-neox-4b\", use_fast=False)\n",
        "model = AutoModelForCausalLM.from_pretrained(\"rinna/bilingual-gpt-neox-4b\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model = model.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsyTU8Vbv05S"
      },
      "outputs": [],
      "source": [
        "# ダウンロードしたファイルを確認、特に指定がない場合はhuggingfaceのモデルは~/.cache/huggingface/hubに保存されます。各インスタンス100GBがディスク容量の上限なので、モデルをダウンロードする前にディスク容量を確認してください。\n",
        "!du -h /root/.cache/huggingface/hub/models--rinna--bilingual-gpt-neox-4b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKn6P8a2v05S"
      },
      "outputs": [],
      "source": [
        "text = \"大規模言語モデルについて説明してください。高校生でも理解できるように噛み砕いて説明してください。\"\n",
        "token_ids = tokenizer.encode(text, add_special_tokens=False, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(\n",
        "        token_ids.to(model.device),\n",
        "        max_new_tokens=300,\n",
        "        min_new_tokens=100,\n",
        "        do_sample=True,\n",
        "        temperature=0.1,\n",
        "        top_p=0.95,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        bos_token_id=tokenizer.bos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "output = tokenizer.decode(output_ids.tolist()[0])\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfi0mwOnv05T"
      },
      "source": [
        "### (補足) API経由でのみ利用可能なモデルを使用\n",
        "Google社のPALM2、OpenAI社のGPTシリーズ、Anthropic社のClaudeなど大規模言語モデルの重みにユーザーはアクセスできないが、API経由でpromptを送信し結果を受け取ることができます。自社で大規模言語モデルの推論サーバーをデプロイ・保守する必要なく手軽に使用することができるのが特徴です。\n",
        "\n",
        "使用には料金が発生することがあり、使用するモデルや入力するpromptの長さ、出力される結果の長さによって料金が変わることがあります。\n",
        "\n",
        "本演習ではOpenAIのAPIを使用してpromptingを試してみましょう。\n",
        "\n",
        "各自OpenAIのアカウントを作成し、APIキーを取得してください。  \n",
        "https://openai.com/blog/openai-api  \n",
        "料金の目安  \n",
        "https://openai.com/pricing#language-models  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-6DIQqZv05T"
      },
      "outputs": [],
      "source": [
        "# copyした.envにAPI Keyを書き込む、terminalから.envへAPI keyを書き込んで保存してください。\n",
        "!cp env.example .env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shFn2agdv05U"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "# OpenAIのAPIを使うためのライブラリ\n",
        "import openai\n",
        "# OpenAIのtokenizerを使うためのライブラリ\n",
        "import tiktoken\n",
        "# API keyの取り扱いにはご注意ください。\n",
        "# https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety\n",
        "# openai.api_key = \"sk-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
        "# .envファイルにAPI keyを書いておくと、以下のように環境変数から読み込めます。\n",
        "load_dotenv()\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "# https://platform.openai.com/docs/models/overview\n",
        "openai_model_name = 'davinci-002'\n",
        "openai_chat_model_name = 'gpt-3.5-turbo'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MnYUjytv05U"
      },
      "outputs": [],
      "source": [
        "prompt = \"大規模言語モデルについて説明してください。高校生でも理解できるように噛み砕いて説明してください。\"\n",
        "# enc = tiktoken.encoding_for_model(openai_model_name)\n",
        "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "encoded_tokens = enc.encode(prompt)\n",
        "print(len(encoded_tokens))\n",
        "print(encoded_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbrMlZijv05V"
      },
      "outputs": [],
      "source": [
        "# 料金が発生します、実行の際はご注意ください。コメントアウトを外して実行すると、APIを使って文章を生成できます。\n",
        "# response = openai.Completion.create(\n",
        "#   model=openai_model_name,\n",
        "#   max_tokens=100,\n",
        "#   temperature=0.1,\n",
        "#   prompt=prompt\n",
        "# )\n",
        "\n",
        "# # Chat Completionの場合(gpt-3.5-turboなど)\n",
        "# chat_response = openai.ChatCompletion.create(\n",
        "#   model=openai_chat_model_name,\n",
        "#   messages=[\n",
        "#         {\"role\": \"user\", \"content\": prompt},\n",
        "#     ]\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbLoPBoIv05V"
      },
      "outputs": [],
      "source": [
        "# response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AikT8lH3v05V"
      },
      "outputs": [],
      "source": [
        "# print(response.choices[0]['text'])\n",
        "\n",
        "# print('入力prompt長: ', response.usage['prompt_tokens'])\n",
        "\n",
        "# print('出力prompt長: ', response.usage['completion_tokens'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkypQ4vav05W"
      },
      "outputs": [],
      "source": [
        "# print(chat_response.choices[0]['message']['content'])\n",
        "\n",
        "# print('入力prompt長: ', chat_response.usage['prompt_tokens'])\n",
        "\n",
        "# print('出力prompt長: ', chat_response.usage['completion_tokens'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFg4OFUnv05W"
      },
      "source": [
        "### Zero-shot / few-shot / CoT promptingでpromptingの効果を実感\n",
        "promptingによって大規模言語モデルの出力が変化することを実感してみましょう。\n",
        "\n",
        "JGLUEのタスクの一つであるJCommonSenseQAを例に、zero-shot / few-shot promptingの効果を確認してみます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdUyp0qlv05W"
      },
      "outputs": [],
      "source": [
        "# 日本語に特化した 60 億パラメータ規模の GPT モデルの構築と評価, https://www.anlp.jp/proceedings/annual_meeting/2023/pdf_dir/H9-4.pdf\n",
        "prompt = \"\"\"\n",
        "[問題]に対する[答え]を[選択肢]の中から選んでください。\n",
        "[問題]:目標や手段や態度を一つに絞り、終始それで押し通そうとすること。また、そのさまを何という?\n",
        "[選択肢]:[剣道, なぎなた, 牡丹槍, 一本槍, 管槍]\n",
        "[答え]:\"\"\"\n",
        "\n",
        "token_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(\n",
        "        token_ids.to(model.device),\n",
        "        max_new_tokens=300,\n",
        "        min_new_tokens=100,\n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        bos_token_id=tokenizer.bos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "output = tokenizer.decode(output_ids.tolist()[0])\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nz_osS0iv05W"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "[問題]に対する[答え]を[選択肢]の中から選んでください。\n",
        "[問題]:会社で一番偉い人はだれ?\n",
        "[選択肢]:[社長, 部長, 人事部, 課長, エントリーシート]\n",
        "[答え]:社長\n",
        "[問題]:顔についていてものを食べるところは?\n",
        "[選択肢]:[鼻, 目, 言葉, 口, 電話]\n",
        "[答え]:口\n",
        "[問題]:町より大きくて県より小さいものは何?\n",
        "[選択肢]:[村, 役場, 市, 郡, 町内]\n",
        "[答え]:市\n",
        "[問題]:目標や手段や態度を一つに絞り、終始それで押し通そうとすること。また、そのさまを何という?\n",
        "[選択肢]:[剣道, なぎなた, 牡丹槍, 一本槍, 管槍]\n",
        "[答え]:\"\"\"\n",
        "\n",
        "token_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(\n",
        "        token_ids.to(model.device),\n",
        "        max_new_tokens=300,\n",
        "        min_new_tokens=100,\n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        bos_token_id=tokenizer.bos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "output = tokenizer.decode(output_ids.tolist()[0])\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0R40R3Lv05W"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiQRBIpkv05X"
      },
      "source": [
        "few-shot promptingによって答えの単語のみ選ばれていることが確認でき、formattingの効果があることがわかります。  \n",
        "次にCoT Promptingを試してみましょう。  \n",
        "Llama2モデルを使用して、CoT promptingが提唱された論文中の例を試してみます。  \n",
        "Llama2モデルの使用にはライセンスの同意の上、申請が必要なので各自申請をお願いします。  \n",
        "https://ai.meta.com/resources/models-and-libraries/llama-downloads/  \n",
        "上記の申請後以下からHuggingFaceでも申請を行う(Meta社への申請のメールアドレスとHuggingFaceのアカウントで登録しているメールアドレスが一致する必要あり)  \n",
        "https://huggingface.co/meta-llama/Llama-2-13b-hf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkgz0RqVv05X"
      },
      "outputs": [],
      "source": [
        "# 自分のHuggingFaceアカウントと紐付ける(申請済みのアカウントでないとモデルをダウンロードできないため)、terminalでhuggingface-cli loginを実行してください。\n",
        "# !huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRi7fXCSv05X"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
        "# 前のセルまでに使用したGPUメモリを解放します。\n",
        "del model\n",
        "del tokenizer\n",
        "torch.cuda.empty_cache()\n",
        "# https://huggingface.co/meta-llama\n",
        "# https://huggingface.co/docs/transformers/model_doc/llama2\n",
        "# https://github.com/facebookresearch/llama-recipes/tree/main\n",
        "# ダウンロードには6分ほどかかります。\n",
        "model_name = 'meta-llama/Llama-2-13b-hf'\n",
        "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
        "\n",
        "model =LlamaForCausalLM.from_pretrained(model_name, load_in_8bit=True, device_map='auto', torch_dtype=torch.float16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReP6HJDEv05X"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
        "A: \"\"\"\n",
        "\n",
        "model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    print(tokenizer.decode(model.generate(**model_input, do_sample=False, max_new_tokens=100)[0], skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnHMPeNBv05Y"
      },
      "outputs": [],
      "source": [
        "# Chain-of-Thought Prompting Elicits Reasoning in Large Language Models: https://arxiv.org/abs/2201.11903\n",
        "prompt = \"\"\"\n",
        "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
        "A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
        "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
        "A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
        "Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
        "A: Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. The answer is 39.\n",
        "Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\n",
        "A: Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. The answer is 8.\n",
        "Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\n",
        "A: Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. The answer is 9.\n",
        "Q: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\n",
        "A: There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. The answer is 29.\n",
        "Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\n",
        "A: Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls. The answer is 33.\n",
        "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
        "A: \"\"\"\n",
        "\n",
        "model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    print(tokenizer.decode(model.generate(**model_input, do_sample=False, max_new_tokens=100)[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtwWeo5dv05Y"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
        "A: Let’s think step by step. \"\"\"\n",
        "\n",
        "model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    print(tokenizer.decode(model.generate(**model_input, do_sample=False, max_new_tokens=200)[0], skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKfvm0ATv05Y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VD1sbIUv05Y"
      },
      "source": [
        "「Let's think step by step」という文言によって思考の流れが出力されるようになりました。  \n",
        "次にprompt中の些細な違いが出力にどのような影響を与えるかを確認してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhPA2BB0v05a"
      },
      "outputs": [],
      "source": [
        "print('############# with space in the end #############')\n",
        "prompt = \"\"\"\n",
        "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
        "A: Let’s think step by step.\"\"\"\n",
        "\n",
        "model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    print(tokenizer.decode(model.generate(**model_input, do_sample=False, max_new_tokens=100)[0], skip_special_tokens=True))\n",
        "\n",
        "print('############# with 全角space in the end #############')\n",
        "prompt = \"\"\"\n",
        "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
        "A: Let’s think step by step.　\"\"\"\n",
        "\n",
        "model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    print(tokenizer.decode(model.generate(**model_input, do_sample=False, max_new_tokens=200)[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOgIPUj7v05a"
      },
      "outputs": [],
      "source": [
        "# 料金が発生します\n",
        "# prompt = \"\"\"\n",
        "# Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
        "# A: Let’s think step by step.\"\"\"\n",
        "# response = openai.Completion.create(\n",
        "#   model=openai_model_name,\n",
        "#   max_tokens=100,\n",
        "#   temperature=0,\n",
        "#   prompt=prompt\n",
        "# )\n",
        "# print(response.choices[0]['text'])\n",
        "# print('入力token長: ', response.usage['prompt_tokens'])\n",
        "# print('出力token長: ', response.usage['completion_tokens'])\n",
        "# print('total token長: ', response.usage['total_tokens'])\n",
        "# print('############# with space in the end #############')\n",
        "# prompt = \"\"\"\n",
        "# Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
        "# A: Let’s think step by step. \"\"\"\n",
        "# response = openai.Completion.create(\n",
        "#   model=openai_model_name,\n",
        "#   max_tokens=100,\n",
        "#   temperature=0,\n",
        "#   prompt=prompt\n",
        "# )\n",
        "# print(response.choices[0]['text'])\n",
        "# print('入力token長: ', response.usage['prompt_tokens'])\n",
        "# print('出力token長: ', response.usage['completion_tokens'])\n",
        "# print('total token長: ', response.usage['total_tokens'])\n",
        "# print('############# with 全角space in the end #############')\n",
        "# prompt = \"\"\"\n",
        "# Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
        "# A: Let’s think step by step.　\"\"\"\n",
        "# response = openai.Completion.create(\n",
        "#   model=openai_model_name,\n",
        "#   max_tokens=100,\n",
        "#   temperature=0,\n",
        "#   prompt=prompt\n",
        "# )\n",
        "# print(response.choices[0]['text'])\n",
        "# print('入力token長: ', response.usage['prompt_tokens'])\n",
        "# print('出力token長: ', response.usage['completion_tokens'])\n",
        "# print('total token長: ', response.usage['total_tokens'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6Ctlaxev05a"
      },
      "outputs": [],
      "source": [
        "# 料金が発生します\n",
        "# prompt = \"\"\"\n",
        "# Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
        "# A: Let’s think step by step.\"\"\"\n",
        "# chat_response = openai.ChatCompletion.create(\n",
        "#   model=openai_chat_model_name,\n",
        "#   temperature=0,\n",
        "#   messages=[\n",
        "#         {\"role\": \"user\", \"content\": prompt},\n",
        "#     ]\n",
        "# )\n",
        "# print(chat_response.choices[0]['message']['content'])\n",
        "# print('入力token長: ', chat_response.usage['prompt_tokens'])\n",
        "# print('出力token長: ', chat_response.usage['completion_tokens'])\n",
        "# print('total token長: ', chat_response.usage['total_tokens'])\n",
        "\n",
        "# print('############# with space in the end #############')\n",
        "# prompt = \"\"\"\n",
        "# Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
        "# A: Let’s think step by step. \"\"\"\n",
        "# chat_response = openai.ChatCompletion.create(\n",
        "#   model=openai_chat_model_name,\n",
        "#   temperature=0,\n",
        "#   messages=[\n",
        "#         {\"role\": \"user\", \"content\": prompt},\n",
        "#     ]\n",
        "# )\n",
        "# print(chat_response.choices[0]['message']['content'])\n",
        "# print('入力token長: ', chat_response.usage['prompt_tokens'])\n",
        "# print('出力token長: ', chat_response.usage['completion_tokens'])\n",
        "# print('total token長: ', chat_response.usage['total_tokens'])\n",
        "\n",
        "# print('############# with 全角space in the end #############')\n",
        "# prompt = \"\"\"\n",
        "# Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
        "# A: Let’s think step by step.　\"\"\"\n",
        "# chat_response = openai.ChatCompletion.create(\n",
        "#   model=openai_chat_model_name,\n",
        "#   temperature=0,\n",
        "#   messages=[\n",
        "#         {\"role\": \"user\", \"content\": prompt},\n",
        "#     ]\n",
        "# )\n",
        "# print(chat_response.choices[0]['message']['content'])\n",
        "# print('入力token長: ', chat_response.usage['prompt_tokens'])\n",
        "# print('出力token長: ', chat_response.usage['completion_tokens'])\n",
        "# print('total token長: ', chat_response.usage['total_tokens'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kT1_ascQv05b"
      },
      "source": [
        "### Prompt engineering guide\n",
        "tokenizerや学習データの違いにより、使用するモデルによって、promptingの影響は異なります。  \n",
        "選定したモデルによってどのような特徴があるのか、そのモデルが提唱された論文や公式ドキュメントを確認することで推し量ることができます。  \n",
        "例えばOpenAI社のGPTシリーズでは以下のような事例集を公開しています。  \n",
        "- https://github.com/openai/openai-cookbook\n",
        "- https://platform.openai.com/docs/guides/gpt-best-practices\n",
        "- https://help.openai.com/en/collections/3675942-prompt-engineering\n",
        "    - https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api\n",
        "\n",
        "以下もおすすめです。  \n",
        "- https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\n",
        "- https://github.com/dair-ai/Prompt-Engineering-Guide\n",
        "- https://github.com/f/awesome-chatgpt-prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7wc4qC0v05b"
      },
      "source": [
        "##  Retrieval Augmented Generation\n",
        "言語モデルへpromptingを行うことで、幅広いタスクに対応するイメージを掴みました。  \n",
        "さらなる活用方法として、言語モデルの論理思考能力をもとにしたサブタスク化を活用する方法や、言語モデル自身の重みだけでなく外部のツール・モデル・情報源を活用する方法があります。  \n",
        "言語モデル単体だけでタスクを行わせる場合と比べて、より難しいタスクに対応できたり、より高い精度でタスクを解かせることができると期待されます。  \n",
        "このような言語モデルの活用を[Augmented Language Model](https://arxiv.org/abs/2302.07842)と呼びます。  \n",
        "Augmented Language Modelの一種であるRetrieval Augmented Generationについて実装し、言語モデルの活用方法についてイメージを掴みましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyT7fMPSv05b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# 前のセルまでに使用したGPUメモリを解放します。 これでも溢れてしまう場合はRestart Kernelを試してください。nvidia-smiコマンドでメモリ使用量を確認できます。\n",
        "# del model\n",
        "# del tokenizer\n",
        "# torch.cuda.empty_cache()\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"rinna/bilingual-gpt-neox-4b\", use_fast=False)\n",
        "model = AutoModelForCausalLM.from_pretrained(\"rinna/bilingual-gpt-neox-4b\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model = model.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCdxDx9Gv05b"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFECVkSxv05c"
      },
      "source": [
        "### Generation without Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Km7Tp_1Sv05c"
      },
      "outputs": [],
      "source": [
        "# In-Context Retrieval-Augmented Language Models: https://arxiv.org/abs/2302.00083\n",
        "prompt = \"\"\"\n",
        "以下の質問に回答してください:\n",
        "質問:　東京大学の松尾研究室が開講する大規模言語モデル講座ではどのような内容を扱いますか？\n",
        "回答:\"\"\"\n",
        "\n",
        "token_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(\n",
        "        token_ids.to(model.device),\n",
        "        max_new_tokens=300,\n",
        "        min_new_tokens=100,\n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        bos_token_id=tokenizer.bos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "output = tokenizer.decode(output_ids.tolist()[0])\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFvh6-i6v05c"
      },
      "source": [
        "### Generation with gold passage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4g-BEOi0v05c"
      },
      "outputs": [],
      "source": [
        "# In-Context Retrieval-Augmented Language Models: https://arxiv.org/abs/2302.00083\n",
        "query = \"東京大学の松尾研究室が開講する大規模言語モデル講座ではどのような内容を扱いますか？\"\n",
        "retrieved_text = \"\"\"\n",
        "【東大松尾研 大規模言語モデル講座開講！】\n",
        " LLMを実装・活用するために必要な知識を扱う無償講座を9/4〜新規開講。GPTの基本的実装からInstruction Tuning/RLHF/高速化等最新のLLMを支える技術まで座学・演習を通じて体系的に学ぶ。募集対象は全国の学生。締切は7月末迄。\n",
        " \"\"\"\n",
        "\n",
        "prompt = f\"\"\"{retrieved_text}\n",
        "上記の文章に基づいて、質問に回答してください。\n",
        "質問: {query}\n",
        "回答:\"\"\"\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KZj6Fxuv05c"
      },
      "outputs": [],
      "source": [
        "token_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(\n",
        "        token_ids.to(model.device),\n",
        "        max_new_tokens=300,\n",
        "        min_new_tokens=100,\n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        bos_token_id=tokenizer.bos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "output = tokenizer.decode(output_ids.tolist()[0])\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpR2v86Jv05i"
      },
      "outputs": [],
      "source": [
        "# OpenAIのAPIを利用した回答。料金がかかります。\n",
        "# prompt = f\"質問: {query}\"\n",
        "# print(prompt)\n",
        "# chat_response = openai.ChatCompletion.create(\n",
        "#   model=openai_chat_model_name,\n",
        "#   messages=[\n",
        "#         {\"role\": \"user\", \"content\": prompt},\n",
        "#     ]\n",
        "# )\n",
        "# print(chat_response.choices[0]['message']['content'])\n",
        "# print(\"############# with retrieved_text #############\")\n",
        "# prompt = f\"\"\"{retrieved_text}\n",
        "# 上記の文章に基づいて、質問に回答してください。\n",
        "# 質問: {query}\n",
        "# \"\"\"\n",
        "# print(prompt)\n",
        "# chat_response = openai.ChatCompletion.create(\n",
        "#   model=openai_chat_model_name,\n",
        "#   messages=[\n",
        "#         {\"role\": \"user\", \"content\": prompt},\n",
        "#     ]\n",
        "# )\n",
        "# print(chat_response.choices[0]['message']['content'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrFjHG57v05j"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3uVeWWiv05j"
      },
      "source": [
        "### Retrieval の実装\n",
        "大規模モデルに質問をしただけでは事実に基づいた正しい回答が得られませんでしたが、回答の根拠となる文章が与えられると、根拠に基づいた回答がなされることを確認できました。\n",
        "\n",
        "質問に応じて根拠となる文章を検索し、回答の際に参照することによって文を生成するのがRetrieval Augmented Generationです。  \n",
        "[松尾研のニュース記事](https://weblab.t.u-tokyo.ac.jp/category/lab-news/)を回答根拠として質問に答える一連の流れを実装してみましょう。  \n",
        "予め収集した記事データを読み込みます。(実装はcollect_news.pyを参照してください。)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeRrJKcRv05j"
      },
      "outputs": [],
      "source": [
        "# 表形式のデータを読み込むためのライブラリ\n",
        "import pandas as pd\n",
        "df = pd.read_json('data.json')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roFBw1VJv05j"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOe6vfYCv05j"
      },
      "source": [
        "どのように回答の根拠となる文章を選択するかにはさまざまな手法があります。  \n",
        "今回は文章をベクトル化するモデルを使用し、ベクトルを元に質問文と記事中の文章の類似度を計算し、類似度が高い文章を選択することで回答の根拠となる文章を選択します。  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvWhTOA_v05j"
      },
      "outputs": [],
      "source": [
        "# 文章や画像のembeddingを扱うためのライブラリ、HuggingFaceのモデルを使うこともできます。\n",
        "from sentence_transformers import SentenceTransformer\n",
        "# https://huggingface.co/intfloat/multilingual-e5-large\n",
        "model = SentenceTransformer('intfloat/multilingual-e5-large')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxG_dF-Lv05j"
      },
      "outputs": [],
      "source": [
        "# 文章を1024次元のベクトルに変換する\n",
        "embeddings = model.encode(\"東京大学の松尾研究室が開講する大規模言語モデル講座ではどのような内容を扱いますか？\", normalize_embeddings=True)\n",
        "print(embeddings.shape)\n",
        "print(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNjkvw9gv05k"
      },
      "outputs": [],
      "source": [
        "input_texts = [\n",
        "    \"passage: 【東大松尾研 大規模言語モデル講座開講！】LLMを実装・活用するために必要な知識を扱う無償講座を9/4〜新規開講。GPTの基本的実装からInstruction Tuning/RLHF/高速化等最新のLLMを支える技術まで座学・演習を通じて体系的に学ぶ。募集対象は全国の学生。締切は7月末迄。\",\n",
        "    \"passage: 【学生限定・受講生募集】9月27日から開講「世界モデルと知能」の受講生を募集します！本講座では，世界モデルを軸に最新の深層学習技術を身につけることができます．深層学習の基礎を習得済みであれば，東大以外の学生も応募可能です！ 締切：9月10日（日）23:59\",\n",
        "    \"passage: 【学生限定・締切間近！】8/25(金)より開講！東大松尾研の金融系PJチームが企画・運営する短期集中講座「金融市場取引と機械学習」の受講生を募集。金融取引に対する機械学習の活用について、理論・実装の両面から学べます。締切:8/7(月)午前10時詳細:\"\n",
        "    ]\n",
        "query_embeddings = model.encode(['query: 東京大学の松尾研究室が開講する大規模言語モデル講座ではどのような内容を扱いますか？'], normalize_embeddings=True)\n",
        "passage_embeddings = model.encode(input_texts, normalize_embeddings=True)\n",
        "print(passage_embeddings.shape)\n",
        "# 類似度の計算\n",
        "scores = (query_embeddings @ passage_embeddings.T) * 100\n",
        "print(scores[0].tolist())\n",
        "# https://numpy.org/doc/stable/reference/generated/numpy.argsort.html\n",
        "# 類似度の高い順のインデックスを取得\n",
        "print(scores[0].argsort()[::-1])\n",
        "# 一番高い類似度の文章を取得\n",
        "print(input_texts[scores[0].argsort()[::-1][0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqtvy1vav05k"
      },
      "outputs": [],
      "source": [
        "input_texts = ['passage: ' + content for content in df.content.tolist()]\n",
        "input_texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhIuCPLPv05k"
      },
      "source": [
        "### Retrieval Augmented Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZquVCZiv05k"
      },
      "outputs": [],
      "source": [
        "query = \"松尾研ではどのようなロボティクス研究をしていますか？\"\n",
        "query_embeddings = model.encode(['query: ' + query], normalize_embeddings=True)\n",
        "passage_embeddings = model.encode(input_texts, normalize_embeddings=True)\n",
        "scores = (query_embeddings @ passage_embeddings.T) * 100\n",
        "# 上位3件を表示\n",
        "print('score: ', scores[0][scores[0].argsort()[::-1][0]])\n",
        "print(input_texts[scores[0].argsort()[::-1][0]])\n",
        "print('score: ', scores[0][scores[0].argsort()[::-1][1]])\n",
        "print(input_texts[scores[0].argsort()[::-1][1]])\n",
        "print('score: ', scores[0][scores[0].argsort()[::-1][2]])\n",
        "print(input_texts[scores[0].argsort()[::-1][2]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pvhWHptv05k"
      },
      "outputs": [],
      "source": [
        "query = \"松尾研ではどのようなロボティクス研究をしていますか？\"\n",
        "query_embeddings = model.encode(['query: ' + query], normalize_embeddings=True)\n",
        "scores = (query_embeddings @ passage_embeddings.T) * 100\n",
        "\n",
        "top_k = 2\n",
        "top_k_idx = scores[0].argsort()[::-1][:top_k]\n",
        "\n",
        "\n",
        "retrieved_text = f\"\"\"\n",
        "{df.content.tolist()[top_k_idx[0]]}\n",
        "\n",
        "{df.content.tolist()[top_k_idx[1]]}\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"{retrieved_text}\n",
        "上記の文章に基づいて、質問に回答してください。\n",
        "質問: {query}\n",
        "回答:\"\"\"\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kySk5a6sv05l"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"rinna/bilingual-gpt-neox-4b\", use_fast=False)\n",
        "model = AutoModelForCausalLM.from_pretrained(\"rinna/bilingual-gpt-neox-4b\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model = model.to(\"cuda\")\n",
        "token_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(\n",
        "        token_ids.to(model.device),\n",
        "        max_new_tokens=300,\n",
        "        min_new_tokens=100,\n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        bos_token_id=tokenizer.bos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "output = tokenizer.decode(output_ids.tolist()[0])\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuXDt9dBv05l"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okQU-0Z1v05l"
      },
      "source": [
        "どのような塊で回答根拠となるデータを蓄積するか、どのように回答根拠を選ぶか、どのように回答を生成するか、というそれぞれのステップで更なる工夫の余地があります。  \n",
        "以下に参考資料を挙げておきます。\n",
        "- https://github.com/openai/chatgpt-retrieval-plugin\n",
        "    - https://github.com/openai/chatgpt-retrieval-plugin#limitations\n",
        "    - https://github.com/openai/chatgpt-retrieval-plugin#future-directions\n",
        "- https://techcommunity.microsoft.com/t5/azure-ai-services-blog/revolutionize-your-enterprise-data-with-chatgpt-next-gen-apps-w/ba-p/3762087\n",
        "    - https://github.com/Azure-Samples/azure-search-openai-demo\n",
        "- https://acl2023-retrieval-lm.github.io/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}